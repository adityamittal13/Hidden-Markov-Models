{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "5be6871b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library Imports\n",
    "\n",
    "from Bio.HMM.MarkovModel import HiddenMarkovModel, MarkovModelBuilder # Building the Markov Model\n",
    "from Bio.Seq import Seq # Defining biological sequences through code\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6094e971",
   "metadata": {},
   "source": [
    "## Making a Basic Markov Model\n",
    "\n",
    "The Markov Model was made to simulate the following image:\n",
    "![example.png](example.png)\n",
    "\n",
    "The transition states are Rainy and Sunny, while the emission states are \"Walk\", \"Shop\", and \"Clean\" (denoted as W, S, and C respectively in the code).\n",
    "\n",
    "First, a MarkovModelBuilder was created using the aforentioned transition and emission probabilities. Then, a HiddenMarkovModel was built to run the Viterbi algorithm on it to determine the optimal hidden state configuration for a particular emission state combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4395b83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionaries\n",
    "transition_alphabet = ['Rainy', 'Sunny']\n",
    "emission_alphabet = ['W', 'S', 'C'] # W - walk, S - shop, C - clean\n",
    "\n",
    "# From the image above:\n",
    "initial_prob = {'Rainy': 0.3, 'Sunny': 0.7}\n",
    "transition_prob = {('Rainy', 'Rainy'): 0.4,\n",
    "                 ('Rainy', 'Sunny'): 0.6,\n",
    "                 ('Sunny', 'Rainy'): 0.2,\n",
    "                 ('Sunny', 'Sunny'): 0.8}\n",
    "emission_prob = {('Rainy', 'W'): 0.1,\n",
    "                 ('Rainy', 'S'): 0.4,\n",
    "                 ('Rainy', 'C'): 0.5,\n",
    "                 ('Sunny', 'W'): 0.6,\n",
    "                 ('Sunny', 'S'): 0.3,\n",
    "                 ('Sunny', 'C'): 0.1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3aa5bda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Markov Model Builder\n",
    "build = MarkovModelBuilder(transition_alphabet, emission_alphabet)\n",
    "build.set_initial_probabilities(initial_prob) # setting initial probabilities\n",
    "build.allow_all_transitions()\n",
    "for key in list(transition_prob.keys()):\n",
    "    build.set_transition_score(key[0], key[1], transition_prob[key]) # adding transition probabilities\n",
    "for key in list(emission_prob.keys()):\n",
    "    build.set_emission_score(key[0], key[1], emission_prob[key]) # adding emission probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ec8576db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing HiddenMarkovModel - last two parameters are pseudocounts\n",
    "model = HiddenMarkovModel(transition_alphabet, emission_alphabet, initial_prob, transition_prob, emission_prob, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d374f162",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Seq('SunnySunnyRainy'), -4.5972020163389145)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.viterbi(Seq('WSC'), transition_alphabet)\n",
    "# (Seq('SunnySunnyRainy'), -4.5972020163389145) - Sunny, Sunny, Rainy is the most optimal configuration with a \n",
    "# probability of 10^(-4.597)=0.00002529297 or 0.0025%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105632aa",
   "metadata": {},
   "source": [
    "## Finding GC-Rich Regions\n",
    "\n",
    "Next, this analysis of HMMs was translated to biology with attempting to differentiate background from promoter regions using the primary nucleotide sequences. This was the Hidden Markov Model represented:\n",
    "![example2.png](example2.png)\n",
    "\n",
    "Note the initial probabilities were defined as 0.5, because there is an equal chance of them being either a background or promoter region at the beginning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "45cd3142",
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_alphabet = ['B', 'P'] # B - background, P - promoter\n",
    "emission_alphabet = ['A', 'T', 'C', 'G']\n",
    "initial_prob = {'B': 0.5, 'P': 0.5}\n",
    "transition_prob = {('B', 'B'): 0.85,\n",
    "                  ('B', 'P'): 0.15,\n",
    "                  ('P', 'P'): 0.75,\n",
    "                  ('P', 'B'): 0.25}\n",
    "emission_prob = {('B', 'A'): 0.25,\n",
    "                ('B', 'T'): 0.25,\n",
    "                ('B', 'C'): 0.25,\n",
    "                ('B', 'G'): 0.25,\n",
    "                ('P', 'A'): 0.15,\n",
    "                ('P', 'T'): 0.13,\n",
    "                ('P', 'C'): 0.42,\n",
    "                ('P', 'G'): 0.3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ebe473bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HiddenMarkovModel(transition_alphabet, emission_alphabet, initial_prob, transition_prob, emission_prob, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "82470aed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Seq('BBBBBBBBBBB'), -17.567574447856494)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.viterbi(Seq('AGTACACTGGT'), transition_alphabet)\n",
    "# (Seq('BBBBBBBBBBB'), -17.567574447856494) - predicted no promoter regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8e9916ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Seq('BBBBBBBB'), -12.921134576003496)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.viterbi(Seq('GCAAATGC'), transition_alphabet)\n",
    "# (Seq('BBBBBBBB'), -12.921134576003496) - predicted no promoter regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6413fb38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Seq('PPPPPPPPBB'), -15.314217188702496)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.viterbi(Seq('GCGCGCGCAA'), transition_alphabet)\n",
    "# (Seq('PPPPPPPPBB'), -15.314217188702496) - predicted promoter regions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f96c8b2",
   "metadata": {},
   "source": [
    "## HMMs with Memory (Dinucleotide Sequences)\n",
    "\n",
    "Our last analysis looks at HMMS with memory where the value of the last nucleotide determines the value of the next nucleotide and what probability it is being selected. HMMs are usually memoryless, so the way to circumvent this is to define 8 transition states of A, C, G, T either being background (-) or promoter (+). Here was the Hidden Markov Model that was used:\n",
    "![example3.png](example3.png)\n",
    "\n",
    "Transition probabilities were taken from Durbin, Eddy et al. 1998. Note that because there is a 75% chance of going from promoter to promoter, 25% from going to promoter to background, 15% from going to background to promoter, and 85% from going from background to background, the probabilities have to be appropriately scaled. For example if the probability of an A+ going to an A is 0.18, then the probability of A+ going to A+ is 0.75x0.18 = 0.135, while the probability of A+ going to A- is 0.25x0.18 = 0.045. \n",
    "\n",
    "Initial probabilities were defined once again as equally likely to happen with 0.125."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d3779244",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_transition_combs(val1, val2, val3, val4):\n",
    "    return [(state1, state2) for state1 in transition_alphabet[val1:val2] for state2 in transition_alphabet[val3:val4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "310c83a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = 'GCGCAAGAGCAT'\n",
    "transition_alphabet = ['A+', 'C+', 'G+', 'T+', 'A-', 'C-', 'G-', 'T-']\n",
    "emission_alphabet = ['A', 'T', 'C', 'G']\n",
    "\n",
    "initial_prob = {key: 0.125 for key in transition_alphabet}\n",
    "transition_prob = {}\n",
    "emission_prob = {}\n",
    "\n",
    "combs_pp = gen_transition_combs(0, 4, 0, 4)\n",
    "combs_pb = gen_transition_combs(0, 4, 4, 8)\n",
    "combs_bp = gen_transition_combs(4, 8, 0, 4)\n",
    "combs_bb = gen_transition_combs(4, 8, 4, 8)\n",
    "all_combs = combs_pp + combs_pb + combs_bp + combs_bb\n",
    "\n",
    "# Transitions numbers from Durbin, Eddy et al. 1998\n",
    "array_promoter = [0.18, 0.274, 0.426, 0.12, 0.171, 0.368, 0.274, 0.188, 0.161, 0.339, 0.375, 0.125, 0.079, 0.355, 0.384, 0.182]\n",
    "vals_pp = [0.75*val for val in array_promoter]\n",
    "vals_pb = [0.25*val for val in array_promoter]\n",
    "array_background = [0.3, 0.205, 0.285, 0.21, 0.322, 0.298, 0.078, 0.302, 0.248, 0.246, 0.298, 0.208, 0.177, 0.239, 0.292, 0.292]\n",
    "vals_bp = [0.15*val for val in array_background]\n",
    "vals_bb = [0.85*val for val in array_background]\n",
    "all_vals = vals_pp + vals_pb + vals_bp + vals_bb\n",
    "\n",
    "for index in range(len(all_combs)):\n",
    "    transition_prob[all_combs[index]] = all_vals[index]\n",
    "\n",
    "for hidden_state in transition_alphabet:\n",
    "    for emission_state in emission_alphabet:\n",
    "        if (hidden_state[0] == emission_state):\n",
    "            emission_prob[(hidden_state, emission_state)] = 1\n",
    "        else:\n",
    "            emission_prob[(hidden_state, emission_state)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e96fe19e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Seq('G+C+G+C-A-A-G-A-G-C-A-T-'), -19.13769949224289)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = HiddenMarkovModel(transition_alphabet, emission_alphabet, initial_prob, transition_prob, emission_prob, 1, 1)\n",
    "model.viterbi(Seq(sequence), transition_alphabet)\n",
    "# (Seq('G+C+G+C-A-A-T-G-A-C-A-G-T-C-A-G-T-G-C-G-A-G-C-A-T-'), -40.299037996610764) - detected promoter regions in the beginning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359ab927",
   "metadata": {},
   "source": [
    "## Viterbi Coding Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1b2c7b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi_algorithm(observations, states, start_p, trans_p, emit_p, table):\n",
    "    V = [{}]\n",
    "    \n",
    "    # Initializes mechanism to store tracebacks and probabilities in future\n",
    "    for st in states:\n",
    "        V[0][st] = {\"prob\": start_p[st] * emit_p[st][observations[0]], \"prev\": None}\n",
    "   \n",
    "    for t in range(1, len(observations)):\n",
    "        V.append({})\n",
    "        for st in states:\n",
    "            # Initialize - multiply by calculated probabilities of the last step and the transition probability for first state\n",
    "            max_tr_prob = V[t - 1][states[0]][\"prob\"] * trans_p[states[0]][st]\n",
    "            prev_st_selected = states[0] # Initialize variable\n",
    "            for prev_st in states[1:]:\n",
    "                # Find the maximum across all states by multiplying calculated probabilities by transition probability\n",
    "                tr_prob = V[t - 1][prev_st][\"prob\"] * trans_p[prev_st][st]\n",
    "                if tr_prob > max_tr_prob:\n",
    "                    max_tr_prob = tr_prob # maximum probability for EACH state\n",
    "                    prev_st_selected = prev_st\n",
    " \n",
    "            # Multiply result by emission probability and store in V\n",
    "            max_prob = max_tr_prob * emit_p[st][observations[t]]\n",
    "            V[t][st] = {\"prob\": max_prob, \"prev\": prev_st_selected}\n",
    "    \n",
    "    if table:\n",
    "        for line in dptable(V):\n",
    "            print(line)\n",
    " \n",
    "    opt = []\n",
    "    max_prob = 0.0\n",
    "    best_st = None\n",
    " \n",
    "    # Traceback and find maximum probability for last state\n",
    "    for st, data in V[-1].items():\n",
    "        if data[\"prob\"] > max_prob:\n",
    "            max_prob = data[\"prob\"]\n",
    "            best_st = st\n",
    "    opt.append(best_st)\n",
    "    previous = best_st\n",
    " \n",
    "    # Traceback to find optimal probabilities for all states - looking at \"prev\" to determine optimal pathway\n",
    "    for t in range(len(V) - 2, -1, -1):\n",
    "        opt.insert(0, V[t + 1][previous][\"prev\"])\n",
    "        previous = V[t + 1][previous][\"prev\"]\n",
    " \n",
    "    print (\" \".join(opt) + \", probability = %s\" % max_prob)\n",
    "\n",
    "def dptable(V):\n",
    "    yield \" \".join((\"%8d\" % i) for i in range(len(V)))\n",
    "    for state in V[0]:\n",
    "        yield \"%.7s: \" % state + \" \".join(\"%.12s\" % (\"%f\" % v[state][\"prob\"]) for v in V)\n",
    "\n",
    "    \n",
    "def structure(d):\n",
    "    structured_d = {}\n",
    "    for key in list(d.keys()):\n",
    "        if (key[0] not in structured_d):\n",
    "            structured_d[key[0]] = {key[1]: d[key]}\n",
    "        else:\n",
    "            structured_d[key[0]][key[1]] = d[key]\n",
    "    return structured_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "26678e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       0        1        2        3        4        5        6        7        8        9       10       11\n",
      "A+: 0.000000 0.000000 0.000000 0.000000 0.000213 0.000029 0.000000 0.000001 0.000000 0.000000 0.000000 0.000000\n",
      "C+: 0.000000 0.031781 0.000000 0.001661 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000\n",
      "G+: 0.125000 0.000000 0.006531 0.000000 0.000000 0.000000 0.000009 0.000000 0.000000 0.000000 0.000000 0.000000\n",
      "T+: 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000\n",
      "A-: 0.000000 0.000000 0.000000 0.000000 0.000151 0.000039 0.000000 0.000002 0.000000 0.000000 0.000000 0.000000\n",
      "C-: 0.000000 0.026137 0.000000 0.000554 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000\n",
      "G-: 0.125000 0.000000 0.002177 0.000000 0.000000 0.000000 0.000009 0.000000 0.000000 0.000000 0.000000 0.000000\n",
      "T-: 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000\n",
      "G+ C+ G+ C- A- A- G- A- G- C- A- T-, probability = 4.882055521997006e-09\n"
     ]
    }
   ],
   "source": [
    "viterbi_algorithm(list(sequence), transition_alphabet, initial_prob, structure(transition_prob), structure(emission_prob), True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8530abf",
   "metadata": {},
   "source": [
    "## Compare Viterbi Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "699e2af5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HiddenMarkovModel Implementation\n",
      "G+C+G+C-A-A-G-A-G-C-A-T-, probability = -19.13769949224289\n",
      "\n",
      "Own Implementation\n",
      "G+ C+ G+ C- A- A- G- A- G- C- A- T-, probability = 4.882055521997006e-09\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(\"HiddenMarkovModel Implementation\")\n",
    "print(model.viterbi(Seq(sequence), transition_alphabet)[0] + \", probability = \" + str(model.viterbi(Seq(sequence), transition_alphabet)[1]))\n",
    "print(\"\\nOwn Implementation\")\n",
    "print(viterbi_algorithm(list(sequence), transition_alphabet, initial_prob, structure(transition_prob), structure(emission_prob), False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "db4e4ef3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['G', 'C', 'G', 'C', 'A', 'A', 'G', 'A', 'G', 'C', 'A', 'T', '']"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.split('[^a-zA-Z]', str(model.viterbi(Seq(sequence), transition_alphabet)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3330c23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
